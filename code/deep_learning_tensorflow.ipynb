{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lsei\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import h5py\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/train.h5' \n",
    "valset_path = '../data/test.h5' \n",
    "model_path='../model/model.ckpt'\n",
    "\n",
    "\n",
    "w=288\n",
    "h=352\n",
    "c=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch=10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "batch_size=16\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNNnet(input_tensor):\n",
    "    # INPUT 288*352*3\n",
    "    with tf.variable_scope('layer1-conv1'):\n",
    "        conv1_weights = tf.get_variable(\"weight\",[5,5,3,32],initializer=tf.truncated_normal_initializer(stddev=0.1)) \n",
    "        conv1_biases = tf.get_variable(\"bias\", [32], initializer=tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.conv2d(input_tensor, conv1_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer2-pool1\"):\n",
    "        pool1 = tf.nn.max_pool(relu1, ksize = [1,2,2,1],strides=[1,2,2,1],padding=\"SAME\") #144*176*32\n",
    "\n",
    "    with tf.variable_scope(\"layer3-conv2\"):\n",
    "        conv2_weights = tf.get_variable(\"weight\",[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv2_biases = tf.get_variable(\"bias\", [64], initializer=tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.conv2d(pool1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu2 = tf.nn.relu(tf.nn.bias_add(conv2, conv2_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer4-pool2\"):\n",
    "        pool2 = tf.nn.max_pool(relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # 72*88*64\n",
    "\n",
    "    with tf.variable_scope(\"layer5-conv3\"):\n",
    "        conv3_weights = tf.get_variable(\"weight\",[3,3,64,128],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv3_biases = tf.get_variable(\"bias\", [128], initializer=tf.constant_initializer(0.0))\n",
    "        conv3 = tf.nn.conv2d(pool2, conv3_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu3 = tf.nn.relu(tf.nn.bias_add(conv3, conv3_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer6-pool3\"):\n",
    "        pool3 = tf.nn.max_pool(relu3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # 36*44*128\n",
    "\n",
    "    with tf.variable_scope(\"layer7-conv4\"):\n",
    "        conv4_weights = tf.get_variable(\"weight\",[3,3,128,128],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        conv4_biases = tf.get_variable(\"bias\", [128], initializer=tf.constant_initializer(0.0))\n",
    "        conv4 = tf.nn.conv2d(pool3, conv4_weights, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        relu4 = tf.nn.relu(tf.nn.bias_add(conv4, conv4_biases))\n",
    "\n",
    "    with tf.name_scope(\"layer8-pool4\"):\n",
    "        pool4 = tf.nn.max_pool(relu4, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') # 18*22*128\n",
    "        nodes = 18*22*128\n",
    "        reshaped = tf.reshape(pool4,[-1, nodes])\n",
    "\n",
    "    with tf.variable_scope('layer9-fc1'):\n",
    "        fc1_weights = tf.get_variable(\"weight\", [nodes, 1024],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "        fc1_biases = tf.get_variable(\"bias\", [1024], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        fc1 = tf.nn.relu(tf.matmul(reshaped, fc1_weights) + fc1_biases)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('layer10-fc2'):\n",
    "        fc2_weights = tf.get_variable(\"weight\", [1024, 512],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "        fc2_biases = tf.get_variable(\"bias\", [512], initializer=tf.constant_initializer(0.1))\n",
    "\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, fc2_weights) + fc2_biases)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('layer11-fc3'):\n",
    "        fc3_weights = tf.get_variable(\"weight\", [512, 2],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "\n",
    "        fc3_biases = tf.get_variable(\"bias\", [2], initializer=tf.constant_initializer(0.1))\n",
    "        net = tf.matmul(fc2, fc3_weights) + fc3_biases\n",
    "        \n",
    "    prediction = tf.nn.softmax(net)\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,shape=[None,w,h,c],name='x')\n",
    "y_=tf.placeholder(tf.int32,shape=[None,],name='y_')\n",
    "y = CNNnet(x)\n",
    "\n",
    "loss=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=y_)\n",
    "train_op=tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(y,1),tf.int32), y_)    \n",
    "acc= tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch(data, label, batch_size):\n",
    "    arr = np.arange(data.shape[0])\n",
    "    np.random.shuffle(arr)\n",
    "    for start_idx in range(0, len(data) - batch_size + 1, batch_size):\n",
    "        chunk = list(arr[start_idx:start_idx + batch_size])\n",
    "        chunk.sort()\n",
    "        yield data[chunk], label[chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 | train loss: 13.272190 | train acc: 0.483750\n",
      "epoch 0 | validation loss: 13.095520 | validation acc: 0.494792\n"
     ]
    }
   ],
   "source": [
    "dataset = h5py.File(dataset_path, 'r')\n",
    "data= dataset['data']\n",
    "label = dataset['label']\n",
    "\n",
    "valset = h5py.File(valset_path, 'r')\n",
    "valdata= valset['data']\n",
    "vallabel = valset['label']\n",
    "\n",
    "saver=tf.train.Saver()\n",
    "sess=tf.Session()  \n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(epoch):\n",
    "    #training\n",
    "    train_loss, train_acc, n_batch = 0, 0, 0\n",
    "    for x_train_a, y_train_a in minibatch(data, label, batch_size):\n",
    "        _,err,ac=sess.run([train_op,loss,acc], feed_dict={x: x_train_a, y_: y_train_a})\n",
    "        train_loss += err; train_acc += ac; n_batch += 1\n",
    "    print(\"epoch %d | train loss: %f | train acc: %f\" % (epoch, np.sum(train_loss)/ n_batch, np.sum(train_acc)/ n_batch))\n",
    "\n",
    "    #validation\n",
    "    val_loss, val_acc, n_batch = 0, 0, 0\n",
    "    for x_val_a, y_val_a in minibatch(valdata, vallabel, batch_size):\n",
    "        err, ac = sess.run([loss,acc], feed_dict={x: x_val_a, y_: y_val_a})\n",
    "        val_loss += err; val_acc += ac; n_batch += 1\n",
    "    print(\"epoch %d | validation loss: %f | validation acc: %f\" % (epoch, np.sum(val_loss)/ n_batch, np.sum(val_acc)/ n_batch))\n",
    "saver.save(sess,model_path)\n",
    "sess.close()\n",
    "\n",
    "dataset.close()\n",
    "valset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
